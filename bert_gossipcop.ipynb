{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & GPU Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# SPECIFY GPU\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning On GossipCOP (Human)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & Pre-Processing\n",
    "- Tokenisation\n",
    "- Analyse tokenised sequence length\n",
    "- Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create tokenised text attention masks & labels in tensor format\n",
    "def pre_process_data(data, tokenizer, max_len):\n",
    "    text = data['text']\n",
    "    labels = data['is_true']\n",
    "\n",
    "    untruncated_tokenised_sequences = [(tokenizer.encode(text, truncation=False, add_special_tokens=True)) for text in text]\n",
    "\n",
    "    tokenised_sequences = tokenizer.batch_encode_plus(\n",
    "    text.tolist(),\n",
    "    max_length = max_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True)\n",
    "\n",
    "    tokenised_sequence_tensor = torch.tensor(tokenised_sequences['input_ids'])\n",
    "    mask_tensor = torch.tensor(tokenised_sequences['attention_mask'])\n",
    "    labels_tensor = torch.tensor(labels.tolist())\n",
    "\n",
    "    return untruncated_tokenised_sequences, tokenised_sequence_tensor, mask_tensor, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (690 > 512). Running this sequence through the model will result in indexing errors\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define tokeniser & truncation max length\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "max_len = 256\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_gossipcop_train.csv\")\n",
    "val_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_gossipcop_validation.csv\")\n",
    "test_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_gossipcop_test.csv\")\n",
    "\n",
    "# Obtain tokenised text, attention masks & labels in tensor format\n",
    "train_untruncated_seq, train_seq, train_mask, train_y = pre_process_data(train_df, tokenizer, max_len)\n",
    "val_untruncated_seq, val_seq, val_mask, val_y = pre_process_data(val_df, tokenizer, max_len)\n",
    "test_untruncated_seq, test_seq, test_mask, test_y = pre_process_data(test_df, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:24835\n",
      "min:6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the histogram of untruncated token counts\n",
    "all_tokenised_seq = train_untruncated_seq + val_untruncated_seq + test_untruncated_seq\n",
    "token_counts = [len(seq) for seq in all_tokenised_seq]\n",
    "print('max:' + str(max(token_counts)))\n",
    "print('min:' + str(min(token_counts)))\n",
    "plt.figure(figsize=(10, 6))\n",
    "pd.Series(token_counts).hist(bins=200, range=(0, 25000))\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Token Counts in Tokenised GossipCop Human Data')\n",
    "plt.xlim(0, 5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define frozen BERT model\n",
    "def initialise_bert_model():\n",
    "    # Load original BERT and freeze its parameters\n",
    "    bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    for param in bert.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Define the model architecture with classification head\n",
    "    # Classification head: ReLu -> FC -> ReLu -> FC -> Softmax\n",
    "    class BERT_Arch(nn.Module):\n",
    "\n",
    "        # Define layers within the model (we use these layers in the forward pass)\n",
    "        def __init__(self, bert):\n",
    "            super(BERT_Arch, self).__init__()\n",
    "            self.bert = bert\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc1 = nn.Linear(768, 512)\n",
    "            self.fc2 = nn.Linear(512, 2)\n",
    "            self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # Define forward pass (flow of data) through network\n",
    "        def forward(self, sent_id, mask):\n",
    "            # Pass the CLS token (BERT's output) to the model\n",
    "            _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "            x = self.fc1(cls_hs)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "\n",
    "    # Create an instance of BERT and place on GPU\n",
    "    model = BERT_Arch(bert)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unfrozen BERT model\n",
    "def initialise_bert_model_unfrozen():\n",
    "    # Load original BERT and freeze its parameters\n",
    "    bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    for param in bert.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Define the model architecture with classification head\n",
    "    # Classification head: ReLu -> FC -> ReLu -> FC -> Softmax\n",
    "    class BERT_Arch(nn.Module):\n",
    "\n",
    "        # Define layers within the model (we use these layers in the forward pass)\n",
    "        def __init__(self, bert):\n",
    "            super(BERT_Arch, self).__init__()\n",
    "            self.bert = bert\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc1 = nn.Linear(768, 512)\n",
    "            self.fc2 = nn.Linear(512, 2)\n",
    "            self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # Define forward pass (flow of data) through network\n",
    "        def forward(self, sent_id, mask):\n",
    "            # Pass the CLS token (BERT's output) to the model\n",
    "            _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "            x = self.fc1(cls_hs)\n",
    "            x = self.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.softmax(x)\n",
    "            return x\n",
    "\n",
    "    # Create an instance of BERT and place on GPU\n",
    "    model = BERT_Arch(bert)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training & Evaluation Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function for BERT model\n",
    "def train(loss_function, optimizer, train_dataloader,model):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        model.zero_grad()\n",
    "        preds = model(sent_id, mask)\n",
    "        batch_loss = loss_function(preds, labels)\n",
    "        total_loss += batch_loss.item()\n",
    "        batch_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "# Define evaluation function for BERT model\n",
    "def evaluate(loss_function, val_dataloader,model):\n",
    "    print(\"\\nEvaluating...\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            batch_loss = loss_function(preds, labels)\n",
    "            total_loss += batch_loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection Using Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nos.remove(model_path)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define path to save / load model weights\n",
    "model_path = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_validation.pt\"\n",
    "\n",
    "# RUN THIS CODE TO RERUN HYPERPARAMETER TUNING\n",
    "'''\n",
    "os.remove(model_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Hyperparameter selection using grid search\n",
    "def hyper_param_selection(learning_rates, batch_sizes, epochs_options, model_inits, train_seq, train_mask, train_y, val_seq, val_mask, val_y):\n",
    "\n",
    "    # Calculate class weights\n",
    "    train_y_numpy = train_y.numpy()\n",
    "    train_y_pd = pd.DataFrame(train_y_numpy)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_y_pd),\n",
    "        y=train_y_pd)\n",
    "    class_weights_tensor= torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    # Initialize variables to store the best model's performance and hyperparameters\n",
    "    best_valid_loss = float('inf')\n",
    "    best_hyperparams = {'lr': None, 'batch_size': None, 'epochs': None, 'model_init': None}\n",
    "\n",
    "    # Check if model weights file already exists\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model weights already saved at {model_path}. Skipping hyperparameter selection.\")\n",
    "    else:\n",
    "        for model_init in model_inits:\n",
    "            for lr in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    for epochs in epochs_options:\n",
    "                        \n",
    "                        if model_init==\"unfrozen\":\n",
    "                            model = initialise_bert_model_unfrozen()\n",
    "                        else:\n",
    "                            model = initialise_bert_model()\n",
    "                        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "                        weighted_cross_entropy = nn.NLLLoss(weight=class_weights_tensor)\n",
    "                        \n",
    "                        print(f\"\\nTraining with model={model_init}, lr={lr}, batch_size={batch_size}, epochs={epochs}\")\n",
    "                        train_dataloader = DataLoader(TensorDataset(train_seq, train_mask, train_y), \n",
    "                                                    sampler=RandomSampler(TensorDataset(train_seq, train_mask, train_y)), \n",
    "                                                    batch_size=batch_size)\n",
    "                        val_dataloader = DataLoader(TensorDataset(val_seq, val_mask, val_y), \n",
    "                                                    sampler=SequentialSampler(TensorDataset(val_seq, val_mask, val_y)), \n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "                        for epoch in range(epochs):\n",
    "                            print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "                            train_loss, _ = train(loss_function=weighted_cross_entropy, optimizer=optimizer, train_dataloader=train_dataloader, model=model)\n",
    "                            valid_loss, _ = evaluate(loss_function=weighted_cross_entropy, val_dataloader=val_dataloader, model=model)\n",
    "                            if valid_loss < best_valid_loss:\n",
    "                                best_valid_loss = valid_loss\n",
    "                                best_hyperparams = {'lr': lr, 'batch_size': batch_size, 'epochs': epochs, 'model_init': model_init}\n",
    "                                torch.save(model.state_dict(), model_path)\n",
    "                                print(f\"Saved new best weights to {model_path}\")\n",
    "                            print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "                            print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "        print(f\"\\nBest Hyperparameters: Model = {best_hyperparams['model_init']}, Learning Rate = {best_hyperparams['lr']}, Batch Size = {best_hyperparams['batch_size']}, Epochs = {best_hyperparams['epochs']}\")\n",
    "        print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights already saved at /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_validation.pt. Skipping hyperparameter selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr': None, 'batch_size': None, 'epochs': None, 'model_init': None}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define hyperparameter search grid\n",
    "learning_rates = [1e-5, 5e-5]\n",
    "batch_sizes = [16]\n",
    "epochs_options = [2, 4]\n",
    "model_inits = [\"unfrozen\", \"frozen\"]\n",
    "\n",
    "# Run hyperparameter selection (this code will not run if model weights already exist as we have already performed this step)\n",
    "hyper_param_selection(learning_rates=learning_rates, batch_sizes=batch_sizes, epochs_options=epochs_options, model_inits=model_inits, train_seq=train_seq, train_mask=train_mask, train_y=train_y, val_seq=val_seq, val_mask=val_mask, val_y=val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Using Chosen Hyperparameters On Train Union Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nos.remove(model_path)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define path to save / load model weights\n",
    "model_path = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_finetuned.pt\"\n",
    "\n",
    "# RUN THIS CODE TO RERUN FINE-TUNING\n",
    "'''\n",
    "os.remove(model_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine-tune the model on chosen hyperparameters\n",
    "def fine_tune(training_sequence, training_mask, training_y, epochs, batch_size, lr, model_init, model_path):\n",
    "    training_y_numpy = training_y.numpy()\n",
    "    training_y_pd = pd.DataFrame(training_y_numpy)\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(training_y_pd),\n",
    "        y=training_y_pd)\n",
    "    weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model weights already saved at {model_path}. Skipping training.\")\n",
    "    else:\n",
    "        # DataLoader for the combined dataset\n",
    "        training_dataloader = DataLoader(TensorDataset(training_sequence, training_mask, training_y), \n",
    "                                        sampler=RandomSampler(TensorDataset(training_sequence, training_mask, training_y)), \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "        if model_init==\"unfrozen\":\n",
    "            model = initialise_bert_model_unfrozen()\n",
    "        else:\n",
    "            model = initialise_bert_model()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        cross_entropy = nn.NLLLoss(weight=weights)\n",
    "\n",
    "        # Model training loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f'\\n Epoch {epoch + 1} / {epochs}')\n",
    "            train_loss = 0\n",
    "            model.train()\n",
    "            for step, batch in enumerate(training_dataloader):\n",
    "                batch = [item.to(device) for item in batch]\n",
    "                seq, mask, labels = batch\n",
    "                model.zero_grad()\n",
    "                outputs = model(seq, mask)\n",
    "                loss = cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Log the average loss of the epoch\n",
    "            avg_train_loss = train_loss / len(training_dataloader)\n",
    "            print(f'\\nAverage Training Loss: {avg_train_loss:.3f}')\n",
    "\n",
    "        # Save the trained model weights\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Model weights saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights already saved at /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_finetuned.pt. Skipping training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Define best hyperparameters as chosen by previous hyperparameter selection grid search\n",
    "best_lr = 1e-5\n",
    "best_batch_size = 16\n",
    "best_epochs = 2\n",
    "best_model_init = \"unfrozen\"\n",
    "\n",
    "# Combine training and validation data for fine-tuning\n",
    "combined_training_sequence = torch.cat((train_seq, val_seq), 0)\n",
    "combined_training_mask = torch.cat((train_mask, val_mask), 0)\n",
    "combined_training_y = torch.cat((train_y, val_y), 0)\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune(training_sequence=combined_training_sequence, training_mask=combined_training_mask, training_y=combined_training_y, epochs=best_epochs, batch_size=best_batch_size, lr=best_lr, model_init=best_model_init, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Labels On Test Set & LLM Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "model=initialise_bert_model_unfrozen()\n",
    "model.load_state_dict(torch.load(model_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict labels and compute performance metrics using batching\n",
    "def predictions_and_metrics_basic(model, original_df, sequences, masks, labels):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    batch_size = 32\n",
    "\n",
    "    n_batches = (len(sequences) + batch_size - 1) // batch_size\n",
    "    print(f\"Total batches to process: {n_batches}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            seq_batch = sequences[i:i+batch_size].to(device)\n",
    "            mask_batch = masks[i:i+batch_size].to(device)\n",
    "            \n",
    "            preds = model(seq_batch, mask_batch)\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "\n",
    "            \n",
    "            # Progress update every 10 batches\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                processed_batches = i // batch_size + 1\n",
    "                remaining_batches = n_batches - processed_batches\n",
    "                print(f\"Processed {processed_batches} batches, {remaining_batches} batches remaining\")\n",
    "    \n",
    "    all_preds=np.concatenate(all_preds, axis=0)\n",
    "    predictions = np.argmax(all_preds, axis=1)\n",
    "\n",
    "    text = original_df['text']\n",
    "    text_pd = pd.DataFrame(text)\n",
    "    text_pd = text_pd.iloc[:,0]\n",
    "\n",
    "    labels_numpy = labels.numpy()\n",
    "    labels_pd = pd.DataFrame(labels_numpy)\n",
    "    labels_pd = labels_pd.iloc[:,0]\n",
    "\n",
    "    results_df = pd.DataFrame({'Text': text_pd, 'Predicted Label': predictions, 'Original Label': labels_pd})\n",
    "    results_df = results_df.dropna(subset=['Text'])\n",
    "\n",
    "    original_misinformations = results_df[results_df['Original Label'] == 0]\n",
    "    correct_predictions = original_misinformations[original_misinformations['Predicted Label'] == 0].shape[0]\n",
    "    success_rate = 100*(correct_predictions / original_misinformations.shape[0]) if original_misinformations.shape[0] > 0 else 0\n",
    "\n",
    "    metrics = classification_report(labels, predictions)\n",
    "    \n",
    "    return results_df, success_rate, metrics\n",
    "\n",
    "\n",
    "# Function to predict labels and compute performance metrics using batching\n",
    "# This function also outputs classwise success rates (for different LLMFake categories)\n",
    "def predictions_and_metrics_classwise(model, original_df, sequences, masks, labels):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    batch_size = 32\n",
    "\n",
    "    n_batches = (len(sequences) + batch_size - 1) // batch_size\n",
    "    print(f\"Total batches to process: {n_batches}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            seq_batch = sequences[i:i+batch_size].to(device)\n",
    "            mask_batch = masks[i:i+batch_size].to(device)\n",
    "            \n",
    "            preds = model(seq_batch, mask_batch)\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "\n",
    "            # Progress update every 10 batches\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                processed_batches = i // batch_size + 1\n",
    "                remaining_batches = n_batches - processed_batches\n",
    "                print(f\"Processed {processed_batches} batches, {remaining_batches} batches remaining\")\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    predictions = np.argmax(all_preds, axis=1)\n",
    "\n",
    "    text = original_df['text']\n",
    "    text_pd = pd.DataFrame(text)\n",
    "    text_pd = text_pd.iloc[:, 0]\n",
    "\n",
    "    labels_numpy = labels.numpy()\n",
    "    labels_pd = pd.DataFrame(labels_numpy)\n",
    "    labels_pd = labels_pd.iloc[:, 0]\n",
    "\n",
    "    categories = original_df['label']\n",
    "    categories_pd = pd.DataFrame(categories)\n",
    "    categories_pd = categories_pd.iloc[:, 0]\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Text': text_pd,\n",
    "        'Category': categories_pd,\n",
    "        'Predicted Label': predictions,\n",
    "        'Original Label': labels_pd\n",
    "    })\n",
    "    results_df = results_df.dropna(subset=['Text'])\n",
    "\n",
    "    # Compute overall success rate\n",
    "    original_misinformations = results_df[results_df['Original Label'] == 0]\n",
    "    correct_predictions = original_misinformations[original_misinformations['Predicted Label'] == 0].shape[0]\n",
    "    success_rate = 100 * (correct_predictions / original_misinformations.shape[0]) if original_misinformations.shape[0] > 0 else 0\n",
    "\n",
    "    # Compute class-wise success rates\n",
    "    classwise_success_rates = {}\n",
    "    for category in results_df['Category'].unique():\n",
    "        category_df = results_df[results_df['Category'] == category]\n",
    "        original_misinformations = category_df[category_df['Original Label'] == 0]\n",
    "        correct_predictions = original_misinformations[original_misinformations['Predicted Label'] == 0].shape[0]\n",
    "        classwise_success_rate = 100 * (correct_predictions / original_misinformations.shape[0]) if original_misinformations.shape[0] > 0 else 0\n",
    "        classwise_success_rates[category] = classwise_success_rate\n",
    "\n",
    "    metrics = classification_report(labels, predictions)\n",
    "    \n",
    "    return results_df, success_rate, classwise_success_rates, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 126\n",
      "Processed 10 batches, 116 batches remaining\n",
      "Processed 20 batches, 106 batches remaining\n",
      "Processed 30 batches, 96 batches remaining\n",
      "Processed 40 batches, 86 batches remaining\n",
      "Processed 50 batches, 76 batches remaining\n",
      "Processed 60 batches, 66 batches remaining\n",
      "Processed 70 batches, 56 batches remaining\n",
      "Processed 80 batches, 46 batches remaining\n",
      "Processed 90 batches, 36 batches remaining\n",
      "Processed 100 batches, 26 batches remaining\n",
      "Processed 110 batches, 16 batches remaining\n",
      "Processed 120 batches, 6 batches remaining\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.70       957\n",
      "           1       0.91      0.88      0.90      3045\n",
      "\n",
      "    accuracy                           0.85      4002\n",
      "   macro avg       0.79      0.81      0.80      4002\n",
      "weighted avg       0.85      0.85      0.85      4002\n",
      "\n",
      "Success rate: 73.77%\n",
      "/Applications/AI/msc_project/predictions/my_gossipcop_test_predictions_human_bert.csv already exists. The DataFrame will not be saved.\n"
     ]
    }
   ],
   "source": [
    "# Human trained BERT on human test set\n",
    "# Obtain performance metrics\n",
    "results_df, success_rate, metrics = predictions_and_metrics_basic(model=model, original_df=test_df, sequences=test_seq, masks=test_mask, labels=test_y)\n",
    "print(metrics)\n",
    "print(f\"Success rate: {success_rate:.2f}%\")\n",
    "\n",
    "# Save predictions to file if the file does not already exist\n",
    "results_table_path = '/Applications/AI/msc_project/predictions/my_gossipcop_test_predictions_human_bert.csv'\n",
    "if os.path.exists(results_table_path):\n",
    "    print(f\"{results_table_path} already exists. The DataFrame will not be saved.\")\n",
    "else:\n",
    "    # Save the results_df to the results_table_path\n",
    "    results_df.to_csv(results_table_path, index=False)\n",
    "    print(f\"Results DataFrame saved to {results_table_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.77      0.87       139\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.77       139\n",
      "   macro avg       0.50      0.38      0.43       139\n",
      "weighted avg       1.00      0.77      0.87       139\n",
      "\n",
      "Success rate: 76.98%\n",
      "{'llm_paraphrase': 76.08695652173914, 'llm_rewritten': 85.1063829787234, 'llm_open_generation': 69.56521739130434}\n",
      "/Applications/AI/msc_project/predictions/my_llm_fake_gossipcop_test_predictions_human_bert.csv already exists. The DataFrame will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Human trained BERT on LLMFake test set\n",
    "# Load & pre-process test data\n",
    "ood_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test.csv\")\n",
    "ood_untruncated_seq, ood_seq, ood_mask, ood_y = pre_process_data(ood_df, tokenizer, max_len)\n",
    "\n",
    "# Obtain performance metrics\n",
    "results_df, success_rate, classwise_success_rates, metrics = predictions_and_metrics_classwise(model=model, original_df=ood_df, sequences=ood_seq, masks=ood_mask, labels=ood_y)\n",
    "print(metrics)\n",
    "print(f\"Success rate: {success_rate:.2f}%\")\n",
    "print(classwise_success_rates)\n",
    "\n",
    "results_table_path = '/Applications/AI/msc_project/predictions/my_llm_fake_gossipcop_test_predictions_human_bert.csv'\n",
    "if os.path.exists(results_table_path):\n",
    "    print(f\"{results_table_path} already exists. The DataFrame will not be saved.\")\n",
    "else:\n",
    "    # Save the results_df to the results_table_path\n",
    "    results_df.to_csv(results_table_path, index=False)\n",
    "    print(f\"Results DataFrame saved to {results_table_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Fine-Tuning On GossipCop (Human & LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & Pre-Processing\n",
    "- Tokenisation\n",
    "- Analyse tokenised sequence length\n",
    "- Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "train_human_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_gossipcop_train.csv\")\n",
    "val_human_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_gossipcop_validation.csv\")\n",
    "test_human_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_gossipcop_test.csv\")\n",
    "train_llm_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_train.csv\")\n",
    "val_llm_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_validation.csv\")\n",
    "test_llm_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test.csv\")\n",
    "\n",
    "# Combine Human & LLMFake datasets\n",
    "filtered_train_llm_df = train_llm_df[['text', 'is_true']]\n",
    "combined_train_df = pd.concat([train_human_df, filtered_train_llm_df], ignore_index=True)\n",
    "filtered_val_llm_df = val_llm_df[['text', 'is_true']]\n",
    "combined_val_df = pd.concat([val_human_df, filtered_val_llm_df], ignore_index=True)\n",
    "filtered_test_llm_df = test_llm_df[['text', 'is_true']]\n",
    "combined_test_df = pd.concat([test_human_df, filtered_test_llm_df], ignore_index=True)\n",
    "\n",
    "# Pre-process data\n",
    "train_untruncated_seq, train_seq, train_mask, train_y = pre_process_data(combined_train_df, tokenizer, max_len)\n",
    "val_untruncated_seq, val_seq, val_mask, val_y = pre_process_data(combined_val_df, tokenizer, max_len)\n",
    "test_untruncated_seq, test_seq, test_mask, test_y = pre_process_data(combined_test_df, tokenizer, max_len)\n",
    "\n",
    "\n",
    "# Plot the histogram of token counts of tokenised text for LLMFake Data\n",
    "train_llm_untruncated_seq, train_llm_seq, train_llm_mask, train_llm_y = pre_process_data(filtered_train_llm_df, tokenizer, max_len)\n",
    "val_llm_untruncated_seq, val_llm_seq, val_llm_mask, val_llm_y = pre_process_data(filtered_val_llm_df, tokenizer, max_len)\n",
    "test_llm_untruncated_seq, test_llm_seq, test_llm_mask, test_llm_y = pre_process_data(filtered_test_llm_df, tokenizer, max_len)\n",
    "all_llm_tokenised_seq = train_llm_untruncated_seq + val_llm_untruncated_seq + test_llm_untruncated_seq\n",
    "llm_token_counts = [len(seq) for seq in all_llm_tokenised_seq]\n",
    "plt.figure(figsize=(10, 6))\n",
    "pd.Series(llm_token_counts).hist(bins=200, range=(0, 25000))\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Token Counts in Tokenised LLMFake GossipCop Data')\n",
    "plt.xlim(0, 5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection Using Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nos.remove(model_path)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model path\n",
    "model_path = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_and_llm_validation.pt\"\n",
    "\n",
    "# RUN THIS CODE TO RERUN HYPERPARAMETER TUNING\n",
    "'''\n",
    "os.remove(model_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights already saved at /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_and_llm_validation.pt. Skipping hyperparameter selection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr': None, 'batch_size': None, 'epochs': None, 'model_init': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define hyperparameter search grid\n",
    "learning_rates = [1e-5, 5e-5]\n",
    "batch_sizes = [16]\n",
    "epochs_options = [2, 4]\n",
    "model_inits = [\"unfrozen\", \"frozen\"]\n",
    "\n",
    "# Run hyperparameter selection (this code will not run if model weights already exist as we have already performed this step)\n",
    "hyper_param_selection(learning_rates=learning_rates, batch_sizes=batch_sizes, epochs_options=epochs_options, model_inits=model_inits, train_seq=train_seq, train_mask=train_mask, train_y=train_y, val_seq=val_seq, val_mask=val_mask, val_y=val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Using Chosen Hyperparameters On Train Union Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nos.remove(model_path)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model path\n",
    "model_path = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_llm_finetuned.pt\"\n",
    "\n",
    "# RUN THIS CODE TO RERUN FINE-TUNING\n",
    "'''\n",
    "os.remove(model_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights already saved at /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_llm_finetuned.pt. Skipping training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Define best hyperparameters as chosen by previous hyperparameter selection grid search\n",
    "best_lr = 1e-5\n",
    "best_batch_size = 16\n",
    "best_epochs = 2\n",
    "best_model_init = \"unfrozen\"\n",
    "\n",
    "# Combine training and validation data for fine-tuning\n",
    "combined_training_sequence = torch.cat((train_seq, val_seq), 0)\n",
    "combined_training_mask = torch.cat((train_mask, val_mask), 0)\n",
    "combined_training_y = torch.cat((train_y, val_y), 0)\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune(training_sequence=combined_training_sequence, training_mask=combined_training_mask, training_y=combined_training_y, epochs=best_epochs, batch_size=best_batch_size, lr=best_lr, model_init=best_model_init, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Labels On Test Set & LLM Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "model=initialise_bert_model_unfrozen()\n",
    "model.load_state_dict(torch.load(model_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 130\n",
      "Processed 10 batches, 120 batches remaining\n",
      "Processed 20 batches, 110 batches remaining\n",
      "Processed 30 batches, 100 batches remaining\n",
      "Processed 40 batches, 90 batches remaining\n",
      "Processed 50 batches, 80 batches remaining\n",
      "Processed 60 batches, 70 batches remaining\n",
      "Processed 70 batches, 60 batches remaining\n",
      "Processed 80 batches, 50 batches remaining\n",
      "Processed 90 batches, 40 batches remaining\n",
      "Processed 100 batches, 30 batches remaining\n",
      "Processed 110 batches, 20 batches remaining\n",
      "Processed 120 batches, 10 batches remaining\n",
      "Processed 130 batches, 0 batches remaining\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.73      1096\n",
      "           1       0.90      0.91      0.91      3045\n",
      "\n",
      "    accuracy                           0.86      4141\n",
      "   macro avg       0.82      0.82      0.82      4141\n",
      "weighted avg       0.86      0.86      0.86      4141\n",
      "\n",
      "Success rate: 68.65%\n",
      "/Applications/AI/msc_project/predictions/my_gossipcop_test_predictions_human_and_llm_bert.csv already exists. The DataFrame will not be saved.\n"
     ]
    }
   ],
   "source": [
    "# Human and LLMFake trained BERT on human test set\n",
    "# Obtain performance metrics\n",
    "results_df, success_rate, metrics = predictions_and_metrics_basic(model=model, original_df=test_df, sequences=test_seq, masks=test_mask, labels=test_y)\n",
    "print(metrics)\n",
    "print(f\"Success rate: {success_rate:.2f}%\")\n",
    "\n",
    "results_table_path = '/Applications/AI/msc_project/predictions/my_gossipcop_test_predictions_human_and_llm_bert.csv'\n",
    "if os.path.exists(results_table_path):\n",
    "    print(f\"{results_table_path} already exists. The DataFrame will not be saved.\")\n",
    "else:\n",
    "    # Save the results_df to the results_table_path\n",
    "    results_df.to_csv(results_table_path, index=False)\n",
    "    print(f\"Results DataFrame saved to {results_table_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       139\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       139\n",
      "   macro avg       0.50      0.49      0.49       139\n",
      "weighted avg       1.00      0.98      0.99       139\n",
      "\n",
      "Success rate: 97.84%\n",
      "{'llm_paraphrase': 95.65217391304348, 'llm_rewritten': 97.87234042553192, 'llm_open_generation': 100.0}\n",
      "/Applications/AI/msc_project/predictions/my_llm_fake_gossipcop_test_predictions_human_and_llm_bert.csv already exists. The DataFrame will not be saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Human and LLMFake trained BERT on LLMFake test set\n",
    "# Load & pre-process test data\n",
    "ood_df = pd.read_csv(\"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test.csv\")\n",
    "ood_untruncated_seq, ood_seq, ood_mask, ood_y = pre_process_data(ood_df, tokenizer, max_len)\n",
    "\n",
    "# Obtain performance metrics\n",
    "results_df, success_rate, classwise_success_rates, metrics = predictions_and_metrics_classwise(model=model, original_df=ood_df, sequences=ood_seq, masks=ood_mask, labels=ood_y)\n",
    "print(metrics)\n",
    "print(f\"Success rate: {success_rate:.2f}%\")\n",
    "print(classwise_success_rates)\n",
    "\n",
    "results_table_path = '/Applications/AI/msc_project/predictions/my_llm_fake_gossipcop_test_predictions_human_and_llm_bert.csv'\n",
    "if os.path.exists(results_table_path):\n",
    "    print(f\"{results_table_path} already exists. The DataFrame will not be saved.\")\n",
    "else:\n",
    "    # Save the results_df to the results_table_path\n",
    "    results_df.to_csv(results_table_path, index=False)\n",
    "    print(f\"Results DataFrame saved to {results_table_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling (Human BERT)\n",
    "- We train and evaluate BERT on 5 different train and test sets to obtain 5 sets of performance metrics for statistical hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_training_models(model_paths, train_processed_seq_list, train_processed_mask_list, train_processed_y_list, model_init, epochs, batch_size, lr):\n",
    "\n",
    "    for i in range(len(model_paths)):\n",
    "\n",
    "        model_path = model_paths[i]\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Model weights already saved at {model_path}. Skipping training.\")\n",
    "            continue\n",
    "\n",
    "        # Convert the list elements to PyTorch tensors before passing\n",
    "        fine_tune(\n",
    "            training_sequence=torch.tensor(train_processed_seq_list[i]),\n",
    "            training_mask=torch.tensor(train_processed_mask_list[i]),\n",
    "            training_y=torch.tensor(train_processed_y_list[i]),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            lr=lr,\n",
    "            model_init=model_init,\n",
    "            model_path=model_path\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pre-process multiple datasets\n",
    "def pre_process_multi_datasplits(data_list,tokenizer,max_len):\n",
    "    processed_seq_list=[]\n",
    "    processed_mask_list=[]\n",
    "    processed_y_list=[]\n",
    "    for i in range(len(data_list)):\n",
    "        dataset = pd.read_csv(data_list[i])\n",
    "        _, dataset_seq, dataset_mask, dataset_y = pre_process_data(dataset, tokenizer, max_len)\n",
    "        processed_seq_list.append(dataset_seq)\n",
    "        processed_mask_list.append(dataset_mask)\n",
    "        processed_y_list.append(dataset_y)\n",
    "\n",
    "    return processed_seq_list, processed_mask_list, processed_y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain performance metrics from multiple models on multiple datasets\n",
    "def obtain_resampled_metrics_lists(models_list, original_df_list, sequences_list, masks_list, labels_list):\n",
    "\n",
    "    success_rate_list=[]\n",
    "    macro_f1_list=[]\n",
    "    for i in range(len(models_list)):\n",
    "        _, success_rate, metrics = predictions_and_metrics_basic(models_list[i],original_df_list[i],sequences_list[i],masks_list[i],labels_list[i])\n",
    "        macro_f1_score = float(re.search(r'^\\s*macro avg\\s+[\\d\\.]+\\s+[\\d\\.]+\\s+([\\d\\.]+)', metrics, re.MULTILINE).group(1))\n",
    "\n",
    "        success_rate_list.append(success_rate)\n",
    "        macro_f1_list.append(macro_f1_score)\n",
    "\n",
    "    return success_rate_list,macro_f1_list\n",
    "\n",
    "# Function to obtain performance metrics from multiple models on multiple datasets\n",
    "# This function also outputs classwise success rates (for different LLMFake categories)\n",
    "def obtain_resampled_metrics_lists_classwise(models_list, original_df_list, sequences_list, masks_list, labels_list):\n",
    "    success_rate_list = []\n",
    "    macro_f1_list = []\n",
    "    all_classwise_success_rates = {}\n",
    "\n",
    "    for i in range(len(models_list)):\n",
    "        _, success_rate, classwise_success_rates, metrics = predictions_and_metrics_classwise(\n",
    "            models_list[i], original_df_list[i], sequences_list[i], masks_list[i], labels_list[i]\n",
    "        )\n",
    "        macro_f1_score = float(re.search(r'^\\s*macro avg\\s+[\\d\\.]+\\s+[\\d\\.]+\\s+([\\d\\.]+)', metrics, re.MULTILINE).group(1))\n",
    "\n",
    "        success_rate_list.append(success_rate)\n",
    "        macro_f1_list.append(macro_f1_score)\n",
    "        \n",
    "        for category, rate in classwise_success_rates.items():\n",
    "            if category not in all_classwise_success_rates:\n",
    "                all_classwise_success_rates[category] = []\n",
    "            all_classwise_success_rates[category].append(rate)\n",
    "\n",
    "    return success_rate_list, macro_f1_list, all_classwise_success_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_sequence=torch.tensor(train_processed_seq_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_mask=torch.tensor(train_processed_mask_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_y=torch.tensor(train_processed_y_list[i]),\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "\n",
      "Average Training Loss: 0.500\n",
      "\n",
      " Epoch 2 / 2\n",
      "\n",
      "Average Training Loss: 0.397\n",
      "Model weights saved to /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_sequence=torch.tensor(train_processed_seq_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_mask=torch.tensor(train_processed_mask_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_y=torch.tensor(train_processed_y_list[i]),\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "\n",
      "Average Training Loss: 0.535\n",
      "\n",
      " Epoch 2 / 2\n",
      "\n",
      "Average Training Loss: 0.408\n",
      "Model weights saved to /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_sequence=torch.tensor(train_processed_seq_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_mask=torch.tensor(train_processed_mask_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_y=torch.tensor(train_processed_y_list[i]),\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "\n",
      "Average Training Loss: 0.522\n",
      "\n",
      " Epoch 2 / 2\n",
      "\n",
      "Average Training Loss: 0.394\n",
      "Model weights saved to /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_sequence=torch.tensor(train_processed_seq_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_mask=torch.tensor(train_processed_mask_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_y=torch.tensor(train_processed_y_list[i]),\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "\n",
      "Average Training Loss: 0.516\n",
      "\n",
      " Epoch 2 / 2\n",
      "\n",
      "Average Training Loss: 0.412\n",
      "Model weights saved to /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_sequence=torch.tensor(train_processed_seq_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_mask=torch.tensor(train_processed_mask_list[i]),\n",
      "/var/folders/dy/5p_bn1m946x3s0bhx_9_t6b80000gn/T/ipykernel_70740/634048028.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  training_y=torch.tensor(train_processed_y_list[i]),\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "\n",
      "Average Training Loss: 0.684\n",
      "\n",
      " Epoch 2 / 2\n",
      "\n",
      "Average Training Loss: 0.701\n",
      "Model weights saved to /Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# Define model weight file paths\n",
    "model_path_1 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv1.pt\"\n",
    "model_path_2 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv2.pt\"\n",
    "model_path_3 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv3.pt\"\n",
    "model_path_4 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv4.pt\"\n",
    "model_path_5 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_cv5.pt\"\n",
    "model_paths = [model_path_1, model_path_2, model_path_3, model_path_4, model_path_5]\n",
    "\n",
    "# RUN THIS CODE TO RERUN RESAMPLED FINE-TUNING EVALUATION\n",
    "'''\n",
    "os.remove(model_path_1)\n",
    "os.remove(model_path_2)\n",
    "os.remove(model_path_3)\n",
    "os.remove(model_path_4)\n",
    "os.remove(model_path_5)\n",
    "'''\n",
    "\n",
    "# Load & pre-process training data\n",
    "train_path_1 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_1.csv\"\n",
    "train_path_2 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_2.csv\"\n",
    "train_path_3 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_3.csv\"\n",
    "train_path_4 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_4.csv\"\n",
    "train_path_5 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_5.csv\"\n",
    "train_splits = [train_path_1, train_path_2, train_path_3, train_path_4, train_path_5]\n",
    "train_processed_seq_list, train_processed_mask_list, train_processed_y_list = pre_process_multi_datasplits(data_list=train_splits, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "# Define best hyperparameters as chosen by previous hyperparameter selection grid search on initial exploration\n",
    "best_lr = 1e-5\n",
    "best_batch_size = 16\n",
    "best_epochs = 2\n",
    "best_model_init = \"unfrozen\"\n",
    "\n",
    "# Fine-tune models\n",
    "resample_training_models(model_paths=model_paths, train_processed_seq_list=train_processed_seq_list, train_processed_mask_list=train_processed_mask_list, train_processed_y_list=train_processed_y_list, model_init=best_model_init, epochs=best_epochs, batch_size=best_batch_size, lr=best_lr)\n",
    "\n",
    "# Initialize and load these models\n",
    "model_1 = initialise_bert_model_unfrozen()\n",
    "model_1.load_state_dict(torch.load(model_path_1), strict=False)\n",
    "model_2 = initialise_bert_model_unfrozen()\n",
    "model_2.load_state_dict(torch.load(model_path_2), strict=False)\n",
    "model_3 = initialise_bert_model_unfrozen()\n",
    "model_3.load_state_dict(torch.load(model_path_3), strict=False)\n",
    "model_4 = initialise_bert_model_unfrozen()\n",
    "model_4.load_state_dict(torch.load(model_path_4), strict=False)\n",
    "model_5 = initialise_bert_model_unfrozen()\n",
    "model_5.load_state_dict(torch.load(model_path_5), strict=False)\n",
    "models_list = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 126\n",
      "Processed 10 batches, 116 batches remaining\n",
      "Processed 20 batches, 106 batches remaining\n",
      "Processed 30 batches, 96 batches remaining\n",
      "Processed 40 batches, 86 batches remaining\n",
      "Processed 50 batches, 76 batches remaining\n",
      "Processed 60 batches, 66 batches remaining\n",
      "Processed 70 batches, 56 batches remaining\n",
      "Processed 80 batches, 46 batches remaining\n",
      "Processed 90 batches, 36 batches remaining\n",
      "Processed 100 batches, 26 batches remaining\n",
      "Processed 110 batches, 16 batches remaining\n",
      "Processed 120 batches, 6 batches remaining\n",
      "Total batches to process: 126\n",
      "Processed 10 batches, 116 batches remaining\n",
      "Processed 20 batches, 106 batches remaining\n",
      "Processed 30 batches, 96 batches remaining\n",
      "Processed 40 batches, 86 batches remaining\n",
      "Processed 50 batches, 76 batches remaining\n",
      "Processed 60 batches, 66 batches remaining\n",
      "Processed 70 batches, 56 batches remaining\n",
      "Processed 80 batches, 46 batches remaining\n",
      "Processed 90 batches, 36 batches remaining\n",
      "Processed 100 batches, 26 batches remaining\n",
      "Processed 110 batches, 16 batches remaining\n",
      "Processed 120 batches, 6 batches remaining\n",
      "Total batches to process: 126\n",
      "Processed 10 batches, 116 batches remaining\n",
      "Processed 20 batches, 106 batches remaining\n",
      "Processed 30 batches, 96 batches remaining\n",
      "Processed 40 batches, 86 batches remaining\n",
      "Processed 50 batches, 76 batches remaining\n",
      "Processed 60 batches, 66 batches remaining\n",
      "Processed 70 batches, 56 batches remaining\n",
      "Processed 80 batches, 46 batches remaining\n",
      "Processed 90 batches, 36 batches remaining\n",
      "Processed 100 batches, 26 batches remaining\n",
      "Processed 110 batches, 16 batches remaining\n",
      "Processed 120 batches, 6 batches remaining\n",
      "Total batches to process: 126\n",
      "Processed 10 batches, 116 batches remaining\n",
      "Processed 20 batches, 106 batches remaining\n",
      "Processed 30 batches, 96 batches remaining\n",
      "Processed 40 batches, 86 batches remaining\n",
      "Processed 50 batches, 76 batches remaining\n",
      "Processed 60 batches, 66 batches remaining\n",
      "Processed 70 batches, 56 batches remaining\n",
      "Processed 80 batches, 46 batches remaining\n",
      "Processed 90 batches, 36 batches remaining\n",
      "Processed 100 batches, 26 batches remaining\n",
      "Processed 110 batches, 16 batches remaining\n",
      "Processed 120 batches, 6 batches remaining\n",
      "Total batches to process: 126\n",
      "Processed 10 batches, 116 batches remaining\n",
      "Processed 20 batches, 106 batches remaining\n",
      "Processed 30 batches, 96 batches remaining\n",
      "Processed 40 batches, 86 batches remaining\n",
      "Processed 50 batches, 76 batches remaining\n",
      "Processed 60 batches, 66 batches remaining\n",
      "Processed 70 batches, 56 batches remaining\n",
      "Processed 80 batches, 46 batches remaining\n",
      "Processed 90 batches, 36 batches remaining\n",
      "Processed 100 batches, 26 batches remaining\n",
      "Processed 110 batches, 16 batches remaining\n",
      "Processed 120 batches, 6 batches remaining\n",
      "[78.54889589905363, 74.55089820359282, 78.18756585879873, 83.38727076591155, 0.0]\n",
      "[0.78, 0.79, 0.77, 0.72, 0.43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# [RESAMPLING] Human trained BERT on human test set\n",
    "test_path_1 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_1.csv\"\n",
    "test_path_2 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_2.csv\"\n",
    "test_path_3 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_3.csv\"\n",
    "test_path_4 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_4.csv\"\n",
    "test_path_5 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_5.csv\"\n",
    "test_splits = [test_path_1, test_path_2, test_path_3, test_path_4, test_path_5]\n",
    "test_processed_seq_list, test_processed_mask_list, test_processed_y_list = pre_process_multi_datasplits(data_list=test_splits, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "original_df_1 = pd.read_csv(test_path_1)\n",
    "original_df_2 = pd.read_csv(test_path_2)\n",
    "original_df_3 = pd.read_csv(test_path_3)\n",
    "original_df_4 = pd.read_csv(test_path_4)\n",
    "original_df_5 = pd.read_csv(test_path_5)\n",
    "original_df_list = [original_df_1, original_df_2, original_df_3, original_df_4, original_df_5]\n",
    "\n",
    "success_rate_list, macro_f1_list = obtain_resampled_metrics_lists(models_list, original_df_list=original_df_list, sequences_list=test_processed_seq_list, masks_list=test_processed_mask_list, labels_list=test_processed_y_list)\n",
    "print(success_rate_list)\n",
    "print(macro_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 5\n",
      "[76.2589928057554, 71.94244604316546, 71.22302158273382, 75.53956834532374, 0.0]\n",
      "{'llm_rewritten': [73.91304347826086, 72.3404255319149, 78.72340425531915, 80.43478260869566, 0.0], 'llm_open_generation': [69.56521739130434, 65.21739130434783, 56.52173913043478, 59.57446808510638, 0.0], 'llm_paraphrase': [85.1063829787234, 78.26086956521739, 78.26086956521739, 86.95652173913044, 0.0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# [RESAMPLED] Human trained BERT on LLMFake test set\n",
    "# Note: macro F1 here doesn't make sense\n",
    "test_path_1 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_1.csv\"\n",
    "test_path_2 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_2.csv\"\n",
    "test_path_3 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_3.csv\"\n",
    "test_path_4 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_4.csv\"\n",
    "test_path_5 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_5.csv\"\n",
    "test_splits = [test_path_1, test_path_2, test_path_3, test_path_4, test_path_5]\n",
    "test_processed_seq_list, test_processed_mask_list, test_processed_y_list = pre_process_multi_datasplits(data_list=test_splits, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "original_df_1 = pd.read_csv(test_path_1)\n",
    "original_df_2 = pd.read_csv(test_path_2)\n",
    "original_df_3 = pd.read_csv(test_path_3)\n",
    "original_df_4 = pd.read_csv(test_path_4)\n",
    "original_df_5 = pd.read_csv(test_path_5)\n",
    "original_df_list = [original_df_1, original_df_2, original_df_3, original_df_4, original_df_5]\n",
    "\n",
    "success_rate_list, _, classwise_success_rates = obtain_resampled_metrics_lists_classwise(models_list, original_df_list=original_df_list, sequences_list=test_processed_seq_list, masks_list=test_processed_mask_list, labels_list=test_processed_y_list)\n",
    "print(success_rate_list)\n",
    "print(classwise_success_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling (Human & LLMFake BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fine-tune multiple models\n",
    "def resample_training_models(model_paths, train_processed_seq_list, train_processed_mask_list, train_processed_y_list, model_init, epochs, batch_size, lr):\n",
    "\n",
    "    for i in range(len(model_paths)):\n",
    "\n",
    "        model_path = model_paths[i]\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Model weights already saved at {model_path}. Skipping training.\")\n",
    "            continue\n",
    "\n",
    "        fine_tune(training_sequence=train_processed_seq_list, training_mask=train_processed_mask_list, training_y=train_processed_y_list, epochs=epochs, batch_size=batch_size, lr=lr, model_init=model_init, model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m best_model_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munfrozen\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Fine-tune models\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mresample_training_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_processed_seq_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_training_sequence_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_processed_mask_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_training_mask_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_processed_y_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_training_y_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_model_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Initialize and load models\u001b[39;00m\n\u001b[1;32m     47\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m initialise_bert_model_unfrozen()\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mresample_training_models\u001b[0;34m(model_paths, train_processed_seq_list, train_processed_mask_list, train_processed_y_list, model_init, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel weights already saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping training.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_processed_seq_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_processed_mask_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_processed_y_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 39\u001b[0m, in \u001b[0;36mfine_tune\u001b[0;34m(training_sequence, training_mask, training_y, epochs, batch_size, lr, model_init, model_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m cross_entropy(outputs, labels)\n\u001b[1;32m     38\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 39\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Log the average loss of the epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni_39/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni_39/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/uni_39/lib/python3.9/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/uni_39/lib/python3.9/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/uni_39/lib/python3.9/site-packages/torch/optim/adamw.py:466\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 466\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define model weight file paths\n",
    "model_path_1 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_and_llm_cv1.pt\"\n",
    "model_path_2 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_and_llm_cv2.pt\"\n",
    "model_path_3 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_and_llm_cv3.pt\"\n",
    "model_path_4 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_and_llm_cv4.pt\"\n",
    "model_path_5 = \"/Applications/AI/msc_project/model_weights/bert_weights_gossipcop_human_and_llm_cv5.pt\"\n",
    "model_paths = [model_path_1, model_path_2, model_path_3, model_path_4, model_path_5]\n",
    "\n",
    "# RUN THIS CODE TO RERUN RESAMPLED FINE-TUNING EVALUATION\n",
    "'''\n",
    "os.remove(model_path_1)\n",
    "os.remove(model_path_2)\n",
    "os.remove(model_path_3)\n",
    "os.remove(model_path_4)\n",
    "os.remove(model_path_5)\n",
    "'''\n",
    "\n",
    "# Load & pre-process training data\n",
    "human_train_path_1 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_1.csv\"\n",
    "human_train_path_2 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_2.csv\"\n",
    "human_train_path_3 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_3.csv\"\n",
    "human_train_path_4 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_4.csv\"\n",
    "human_train_path_5 = \"/Applications/AI/msc_project/data/my_human_gossipcop_train_split_5.csv\"\n",
    "human_train_splits = [train_path_1, train_path_2, train_path_3, train_path_4, train_path_5]\n",
    "human_train_processed_seq_list, human_train_processed_mask_list, human_train_processed_y_list = pre_process_multi_datasplits(data_list=human_train_splits, tokenizer=tokenizer, max_len=max_len)\n",
    "llm_train_path_1 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_train_split_1.csv\"\n",
    "llm_train_path_2 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_train_split_2.csv\"\n",
    "llm_train_path_3 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_train_split_3.csv\"\n",
    "llm_train_path_4 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_train_split_4.csv\"\n",
    "llm_train_path_5 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_train_split_5.csv\"\n",
    "llm_train_splits = [llm_train_path_1, llm_train_path_2, llm_train_path_3, llm_train_path_4, llm_train_path_5]\n",
    "llm_train_processed_seq_list, llm_train_processed_mask_list, llm_train_processed_y_list = pre_process_multi_datasplits(data_list=llm_train_splits, tokenizer=tokenizer, max_len=max_len)\n",
    "combined_training_sequence_list = torch.cat((train_seq, val_seq), 0)\n",
    "combined_training_mask_list = torch.cat((train_mask, val_mask), 0)\n",
    "combined_training_y_list = torch.cat((train_y, val_y), 0)\n",
    "\n",
    "# Define best hyperparameters as chosen by previous hyperparameter selection grid search on initial exploration\n",
    "best_lr = 1e-5\n",
    "best_batch_size = 16\n",
    "best_epochs = 2\n",
    "best_model_init = \"unfrozen\"\n",
    "\n",
    "# Fine-tune models\n",
    "resample_training_models(model_paths=model_paths, train_processed_seq_list=combined_training_sequence_list, train_processed_mask_list=combined_training_mask_list, train_processed_y_list=combined_training_y_list, model_init=best_model_init, epochs=best_epochs, batch_size=best_batch_size, lr=best_lr)\n",
    "\n",
    "# Initialize and load models\n",
    "model_1 = initialise_bert_model_unfrozen()\n",
    "model_1.load_state_dict(torch.load(model_path_1), strict=False)\n",
    "model_2 = initialise_bert_model_unfrozen()\n",
    "model_2.load_state_dict(torch.load(model_path_2), strict=False)\n",
    "model_3 = initialise_bert_model_unfrozen()\n",
    "model_3.load_state_dict(torch.load(model_path_3), strict=False)\n",
    "model_4 = initialise_bert_model_unfrozen()\n",
    "model_4.load_state_dict(torch.load(model_path_4), strict=False)\n",
    "model_5 = initialise_bert_model_unfrozen()\n",
    "model_5.load_state_dict(torch.load(model_path_5), strict=False)\n",
    "models_list = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 33\n",
      "Processed 10 batches, 23 batches remaining\n",
      "Processed 20 batches, 13 batches remaining\n",
      "Processed 30 batches, 3 batches remaining\n",
      "Total batches to process: 33\n",
      "Processed 10 batches, 23 batches remaining\n",
      "Processed 20 batches, 13 batches remaining\n",
      "Processed 30 batches, 3 batches remaining\n",
      "Total batches to process: 33\n",
      "Processed 10 batches, 23 batches remaining\n",
      "Processed 20 batches, 13 batches remaining\n",
      "Processed 30 batches, 3 batches remaining\n",
      "Total batches to process: 33\n",
      "Processed 10 batches, 23 batches remaining\n",
      "Processed 20 batches, 13 batches remaining\n",
      "Processed 30 batches, 3 batches remaining\n",
      "Total batches to process: 33\n",
      "Processed 10 batches, 23 batches remaining\n",
      "Processed 20 batches, 13 batches remaining\n",
      "Processed 30 batches, 3 batches remaining\n",
      "[99.47089947089947, 97.79005524861878, 99.45054945054946, 96.21621621621622, 99.43820224719101]\n",
      "[0.99, 0.98, 0.97, 0.98, 0.98]\n"
     ]
    }
   ],
   "source": [
    "# [RESAMPLING] Human & LLMFake trained BERT on human test set\n",
    "test_path_1 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_1.csv\"\n",
    "test_path_2 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_2.csv\"\n",
    "test_path_3 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_3.csv\"\n",
    "test_path_4 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_4.csv\"\n",
    "test_path_5 = \"/Applications/AI/msc_project/data/my_human_gossipcop_test_split_5.csv\"\n",
    "test_splits = [test_path_1, test_path_2, test_path_3, test_path_4, test_path_5]\n",
    "test_processed_seq_list, test_processed_mask_list, test_processed_y_list = pre_process_multi_datasplits(data_list=test_splits, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "original_df_1 = pd.read_csv(test_path_1)\n",
    "original_df_2 = pd.read_csv(test_path_2)\n",
    "original_df_3 = pd.read_csv(test_path_3)\n",
    "original_df_4 = pd.read_csv(test_path_4)\n",
    "original_df_5 = pd.read_csv(test_path_5)\n",
    "original_df_list = [original_df_1, original_df_2, original_df_3, original_df_4, original_df_5]\n",
    "\n",
    "success_rate_list, macro_f1_list = obtain_resampled_metrics_lists(models_list, original_df_list=original_df_list, sequences_list=test_processed_seq_list, masks_list=test_processed_mask_list, labels_list=test_processed_y_list)\n",
    "print(success_rate_list)\n",
    "print(macro_f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/Users/jacobshort/anaconda3/envs/uni_39/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches to process: 2\n",
      "Total batches to process: 2\n",
      "Total batches to process: 2\n",
      "Total batches to process: 2\n",
      "Total batches to process: 2\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "{'llm_open_generation': [100.0, 100.0, 100.0, 100.0, 100.0], 'llm_paraphrase': [100.0, 100.0, 100.0, 100.0, 100.0], 'llm_rewritten': [100.0, 100.0, 100.0, 100.0, 100.0]}\n"
     ]
    }
   ],
   "source": [
    "# [RESAMPLED] Human & LLMFake trained BERT on LLMFake test set\n",
    "# Note: macro F1 here doesn't make sense\n",
    "test_path_1 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_1.csv\"\n",
    "test_path_2 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_2.csv\"\n",
    "test_path_3 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_3.csv\"\n",
    "test_path_4 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_4.csv\"\n",
    "test_path_5 = \"/Applications/AI/msc_project/data/my_llm_fake_gossipcop_test_split_5.csv\"\n",
    "test_splits = [test_path_1, test_path_2, test_path_3, test_path_4, test_path_5]\n",
    "test_processed_seq_list, test_processed_mask_list, test_processed_y_list = pre_process_multi_datasplits(data_list=test_splits, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "original_df_1 = pd.read_csv(test_path_1)\n",
    "original_df_2 = pd.read_csv(test_path_2)\n",
    "original_df_3 = pd.read_csv(test_path_3)\n",
    "original_df_4 = pd.read_csv(test_path_4)\n",
    "original_df_5 = pd.read_csv(test_path_5)\n",
    "original_df_list = [original_df_1, original_df_2, original_df_3, original_df_4, original_df_5]\n",
    "\n",
    "success_rate_list, _, classwise_success_rates = obtain_resampled_metrics_lists_classwise(models_list, original_df_list=original_df_list, sequences_list=test_processed_seq_list, masks_list=test_processed_mask_list, labels_list=test_processed_y_list)\n",
    "print(success_rate_list)\n",
    "print(classwise_success_rates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('uni_39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2442b4eed952f48f926d303071082d30c5080f089ed43bdeb0e861e3760da25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
